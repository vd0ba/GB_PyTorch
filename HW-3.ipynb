{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Домашняя работа №3\n",
    "\n",
    "1. Создать Dataset для загрузки данных (sklearn.datasets.fetch_california_housing)\n",
    "2. Обернуть его в Dataloader\n",
    "3. Написать архитектуру сети, которая предсказывает стоимость недвижимости. Сеть должна включать BatchNorm слои и Dropout (или НЕ включать, но нужно обосновать)\n",
    "4. Сравните сходимость Adam, RMSProp и SGD, сделайте вывод по качеству работы модели train-test разделение нужно сделать с помощью sklearn random_state=13, test_size = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import optim\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaliforniaDataset(torch.utils.data.Dataset):\n",
    "   \n",
    "    def __init__(self, init_dataset, init_target, transform=None):\n",
    "        self._base_dataset = torch.from_numpy(init_dataset).type(torch.float)\n",
    "        self._base_targets = torch.from_numpy(init_target).type(torch.float)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._base_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = self._base_dataset[idx]\n",
    "        target = self._base_targets[idx]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            features = self.transform(features)\n",
    "      \n",
    "        return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, activation=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        if self.activation == \"relu\":\n",
    "            return F.relu(x)\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return F.sigmoid(x)\n",
    "        if self.activation == \"leaky_relu\":\n",
    "            return F.leaky_relu(x)\n",
    "        raise RuntimeError\n",
    "        \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = Perceptron(input_dim, hidden_dim, 'leaky_relu')\n",
    "        self.bn = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dp = nn.Dropout(0.15)\n",
    "        self.fc2 = Perceptron(hidden_dim, 1, 'leaky_relu')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dp(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.fetch_california_housing()\n",
    "     \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['data'], data['target'], test_size = 0.25, random_state = 13)\n",
    "     \n",
    "\n",
    "train_dataset = CaliforniaDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=150, shuffle=False)\n",
    "     \n",
    "\n",
    "test_dataset = CaliforniaDataset(X_test, y_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False)\n",
    "     \n",
    "\n",
    "net = FeedForward(8, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(opt='adam', num_epochs = 20):\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    if opt == 'adam':\n",
    "            optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "    elif opt == 'rmsprop':\n",
    "            optimizer = optim.RMSprop(net.parameters(), lr=0.001)\n",
    "    elif opt == 'sgd':\n",
    "            optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "    for epoch in range(num_epochs):  \n",
    "        running_loss, running_items, r2 = 0.0, 0.0, 0.0\n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "            fets, target = data[0], data[1]\n",
    "\n",
    "            # обнуляем градиент\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(fets)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_items += len(target)\n",
    "\n",
    "            predict = outputs.data.numpy()\n",
    "            tr_target = target.view(target.shape[0], 1).numpy()\n",
    "            r2 += r2_score(tr_target, predict)\n",
    "\n",
    "            if i % 30 == 0:\n",
    "                net.eval()\n",
    "\n",
    "                data = list(test_loader)[0]\n",
    "\n",
    "                test_outputs = net(data[0])\n",
    "                test_predict = test_outputs.data.numpy()\n",
    "                te_target = data[1].view(data[1].shape[0], 1)\n",
    "                test_r2 = r2_score(te_target, test_predict)\n",
    "\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}]. ' \\\n",
    "                      f'Step [{i + 1}/{len(train_loader)}]. ' \\\n",
    "                      f'Loss: {running_loss / running_items:.3f}. ' \\\n",
    "                      f'r2: {r2:.3f}. ' \\\n",
    "                      f'Test r2: {test_r2:.3f}')\n",
    "\n",
    "                running_loss, running_items, r2 = 0.0, 0.0, 0.0\n",
    "\n",
    "                net.train()\n",
    "\n",
    "    print('Training is finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]. Step [1/104]. Loss: 0.041. r2: -3.086. Test r2: -5.301\n",
      "Epoch [1/10]. Step [31/104]. Loss: 0.036. r2: -98.596. Test r2: -5.263\n",
      "Epoch [1/10]. Step [61/104]. Loss: 0.036. r2: -93.125. Test r2: -5.072\n",
      "Epoch [1/10]. Step [91/104]. Loss: 0.035. r2: -90.287. Test r2: -4.236\n",
      "Epoch [2/10]. Step [1/104]. Loss: 0.036. r2: -2.640. Test r2: -3.703\n",
      "Epoch [2/10]. Step [31/104]. Loss: 0.031. r2: -82.019. Test r2: -3.781\n",
      "Epoch [2/10]. Step [61/104]. Loss: 0.029. r2: -69.174. Test r2: -4.191\n",
      "Epoch [2/10]. Step [91/104]. Loss: 0.024. r2: -53.114. Test r2: -2.858\n",
      "Epoch [3/10]. Step [1/104]. Loss: 0.022. r2: -1.136. Test r2: -2.415\n",
      "Epoch [3/10]. Step [31/104]. Loss: 0.019. r2: -35.312. Test r2: -1.362\n",
      "Epoch [3/10]. Step [61/104]. Loss: 0.017. r2: -25.928. Test r2: -0.836\n",
      "Epoch [3/10]. Step [91/104]. Loss: 0.016. r2: -23.776. Test r2: -0.372\n",
      "Epoch [4/10]. Step [1/104]. Loss: 0.016. r2: -0.499. Test r2: -0.884\n",
      "Epoch [4/10]. Step [31/104]. Loss: 0.015. r2: -21.472. Test r2: -0.418\n",
      "Epoch [4/10]. Step [61/104]. Loss: 0.015. r2: -19.024. Test r2: -0.297\n",
      "Epoch [4/10]. Step [91/104]. Loss: 0.014. r2: -19.224. Test r2: -0.408\n",
      "Epoch [5/10]. Step [1/104]. Loss: 0.015. r2: -0.358. Test r2: -0.736\n",
      "Epoch [5/10]. Step [31/104]. Loss: 0.014. r2: -17.697. Test r2: -0.150\n",
      "Epoch [5/10]. Step [61/104]. Loss: 0.014. r2: -18.193. Test r2: -0.164\n",
      "Epoch [5/10]. Step [91/104]. Loss: 0.014. r2: -16.858. Test r2: -0.037\n",
      "Epoch [6/10]. Step [1/104]. Loss: 0.015. r2: -0.498. Test r2: -0.076\n",
      "Epoch [6/10]. Step [31/104]. Loss: 0.013. r2: -15.852. Test r2: -0.130\n",
      "Epoch [6/10]. Step [61/104]. Loss: 0.014. r2: -16.503. Test r2: -0.076\n",
      "Epoch [6/10]. Step [91/104]. Loss: 0.014. r2: -16.169. Test r2: -0.040\n",
      "Epoch [7/10]. Step [1/104]. Loss: 0.014. r2: -0.422. Test r2: -0.081\n",
      "Epoch [7/10]. Step [31/104]. Loss: 0.013. r2: -18.692. Test r2: -0.239\n",
      "Epoch [7/10]. Step [61/104]. Loss: 0.014. r2: -17.554. Test r2: -0.064\n",
      "Epoch [7/10]. Step [91/104]. Loss: 0.013. r2: -16.764. Test r2: -0.031\n",
      "Epoch [8/10]. Step [1/104]. Loss: 0.016. r2: -0.491. Test r2: -0.090\n",
      "Epoch [8/10]. Step [31/104]. Loss: 0.013. r2: -15.703. Test r2: -0.067\n",
      "Epoch [8/10]. Step [61/104]. Loss: 0.011. r2: -7.768. Test r2: -0.101\n",
      "Epoch [8/10]. Step [91/104]. Loss: 0.010. r2: -4.713. Test r2: -0.160\n",
      "Epoch [9/10]. Step [1/104]. Loss: 0.011. r2: -0.004. Test r2: -0.268\n",
      "Epoch [9/10]. Step [31/104]. Loss: 0.010. r2: -3.908. Test r2: -0.235\n",
      "Epoch [9/10]. Step [61/104]. Loss: 0.010. r2: -4.095. Test r2: -0.205\n",
      "Epoch [9/10]. Step [91/104]. Loss: 0.010. r2: -3.968. Test r2: -0.195\n",
      "Epoch [10/10]. Step [1/104]. Loss: 0.011. r2: -0.058. Test r2: -0.310\n",
      "Epoch [10/10]. Step [31/104]. Loss: 0.010. r2: -3.498. Test r2: -0.206\n",
      "Epoch [10/10]. Step [61/104]. Loss: 0.010. r2: -3.416. Test r2: -0.168\n",
      "Epoch [10/10]. Step [91/104]. Loss: 0.010. r2: -4.098. Test r2: -0.128\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "train_nn(opt='adam', num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]. Step [1/104]. Loss: 0.011. r2: -0.075. Test r2: -1.002\n",
      "Epoch [1/10]. Step [31/104]. Loss: 0.010. r2: -3.307. Test r2: -0.207\n",
      "Epoch [1/10]. Step [61/104]. Loss: 0.010. r2: -2.832. Test r2: -0.133\n",
      "Epoch [1/10]. Step [91/104]. Loss: 0.009. r2: -2.611. Test r2: -0.162\n",
      "Epoch [2/10]. Step [1/104]. Loss: 0.011. r2: -0.096. Test r2: -0.242\n",
      "Epoch [2/10]. Step [31/104]. Loss: 0.009. r2: -2.156. Test r2: -0.128\n",
      "Epoch [2/10]. Step [61/104]. Loss: 0.009. r2: -2.263. Test r2: -0.163\n",
      "Epoch [2/10]. Step [91/104]. Loss: 0.009. r2: -2.058. Test r2: -0.152\n",
      "Epoch [3/10]. Step [1/104]. Loss: 0.010. r2: -0.005. Test r2: -0.271\n",
      "Epoch [3/10]. Step [31/104]. Loss: 0.009. r2: -1.759. Test r2: -0.191\n",
      "Epoch [3/10]. Step [61/104]. Loss: 0.009. r2: -1.388. Test r2: -0.178\n",
      "Epoch [3/10]. Step [91/104]. Loss: 0.009. r2: -1.284. Test r2: -0.182\n",
      "Epoch [4/10]. Step [1/104]. Loss: 0.010. r2: -0.031. Test r2: -0.245\n",
      "Epoch [4/10]. Step [31/104]. Loss: 0.009. r2: -1.826. Test r2: -0.177\n",
      "Epoch [4/10]. Step [61/104]. Loss: 0.009. r2: -1.099. Test r2: -0.195\n",
      "Epoch [4/10]. Step [91/104]. Loss: 0.009. r2: -1.237. Test r2: -0.200\n",
      "Epoch [5/10]. Step [1/104]. Loss: 0.011. r2: -0.145. Test r2: -0.248\n",
      "Epoch [5/10]. Step [31/104]. Loss: 0.009. r2: -1.152. Test r2: -0.174\n",
      "Epoch [5/10]. Step [61/104]. Loss: 0.009. r2: -1.063. Test r2: -0.204\n",
      "Epoch [5/10]. Step [91/104]. Loss: 0.009. r2: -1.054. Test r2: -0.207\n",
      "Epoch [6/10]. Step [1/104]. Loss: 0.010. r2: -0.025. Test r2: -0.292\n",
      "Epoch [6/10]. Step [31/104]. Loss: 0.009. r2: -1.053. Test r2: -0.185\n",
      "Epoch [6/10]. Step [61/104]. Loss: 0.009. r2: -0.874. Test r2: -0.193\n",
      "Epoch [6/10]. Step [91/104]. Loss: 0.009. r2: -0.716. Test r2: -0.200\n",
      "Epoch [7/10]. Step [1/104]. Loss: 0.010. r2: -0.005. Test r2: -0.274\n",
      "Epoch [7/10]. Step [31/104]. Loss: 0.009. r2: -0.557. Test r2: -0.184\n",
      "Epoch [7/10]. Step [61/104]. Loss: 0.009. r2: -0.644. Test r2: -0.205\n",
      "Epoch [7/10]. Step [91/104]. Loss: 0.009. r2: -0.765. Test r2: -0.237\n",
      "Epoch [8/10]. Step [1/104]. Loss: 0.010. r2: -0.028. Test r2: -0.287\n",
      "Epoch [8/10]. Step [31/104]. Loss: 0.009. r2: -0.354. Test r2: -0.202\n",
      "Epoch [8/10]. Step [61/104]. Loss: 0.009. r2: -0.542. Test r2: -0.220\n",
      "Epoch [8/10]. Step [91/104]. Loss: 0.009. r2: -0.351. Test r2: -0.239\n",
      "Epoch [9/10]. Step [1/104]. Loss: 0.010. r2: -0.043. Test r2: -0.294\n",
      "Epoch [9/10]. Step [31/104]. Loss: 0.009. r2: -0.299. Test r2: -0.208\n",
      "Epoch [9/10]. Step [61/104]. Loss: 0.009. r2: -0.565. Test r2: -0.221\n",
      "Epoch [9/10]. Step [91/104]. Loss: 0.009. r2: -0.351. Test r2: -0.231\n",
      "Epoch [10/10]. Step [1/104]. Loss: 0.010. r2: 0.001. Test r2: -0.293\n",
      "Epoch [10/10]. Step [31/104]. Loss: 0.009. r2: -0.271. Test r2: -0.207\n",
      "Epoch [10/10]. Step [61/104]. Loss: 0.009. r2: -0.489. Test r2: -0.220\n",
      "Epoch [10/10]. Step [91/104]. Loss: 0.009. r2: -0.296. Test r2: -0.230\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "train_nn(opt='rmsprop', num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]. Step [1/104]. Loss: 0.010. r2: 0.005. Test r2: -0.289\n",
      "Epoch [1/10]. Step [31/104]. Loss: 0.009. r2: -0.331. Test r2: -0.265\n",
      "Epoch [1/10]. Step [61/104]. Loss: 0.009. r2: -0.268. Test r2: -0.243\n",
      "Epoch [1/10]. Step [91/104]. Loss: 0.009. r2: -0.240. Test r2: -0.242\n",
      "Epoch [2/10]. Step [1/104]. Loss: 0.010. r2: 0.003. Test r2: -0.251\n",
      "Epoch [2/10]. Step [31/104]. Loss: 0.009. r2: -0.308. Test r2: -0.240\n",
      "Epoch [2/10]. Step [61/104]. Loss: 0.009. r2: -0.273. Test r2: -0.237\n",
      "Epoch [2/10]. Step [91/104]. Loss: 0.009. r2: -0.220. Test r2: -0.223\n",
      "Epoch [3/10]. Step [1/104]. Loss: 0.010. r2: -0.009. Test r2: -0.240\n",
      "Epoch [3/10]. Step [31/104]. Loss: 0.009. r2: -0.181. Test r2: -0.233\n",
      "Epoch [3/10]. Step [61/104]. Loss: 0.009. r2: -0.414. Test r2: -0.231\n",
      "Epoch [3/10]. Step [91/104]. Loss: 0.009. r2: -0.306. Test r2: -0.228\n",
      "Epoch [4/10]. Step [1/104]. Loss: 0.010. r2: 0.005. Test r2: -0.235\n",
      "Epoch [4/10]. Step [31/104]. Loss: 0.009. r2: -0.241. Test r2: -0.229\n",
      "Epoch [4/10]. Step [61/104]. Loss: 0.009. r2: -0.259. Test r2: -0.231\n",
      "Epoch [4/10]. Step [91/104]. Loss: 0.009. r2: -0.279. Test r2: -0.226\n",
      "Epoch [5/10]. Step [1/104]. Loss: 0.010. r2: -0.007. Test r2: -0.237\n",
      "Epoch [5/10]. Step [31/104]. Loss: 0.009. r2: -0.312. Test r2: -0.228\n",
      "Epoch [5/10]. Step [61/104]. Loss: 0.009. r2: -0.254. Test r2: -0.223\n",
      "Epoch [5/10]. Step [91/104]. Loss: 0.009. r2: -0.209. Test r2: -0.223\n",
      "Epoch [6/10]. Step [1/104]. Loss: 0.010. r2: -0.004. Test r2: -0.236\n",
      "Epoch [6/10]. Step [31/104]. Loss: 0.009. r2: -0.320. Test r2: -0.231\n",
      "Epoch [6/10]. Step [61/104]. Loss: 0.009. r2: -0.421. Test r2: -0.229\n",
      "Epoch [6/10]. Step [91/104]. Loss: 0.009. r2: -0.229. Test r2: -0.226\n",
      "Epoch [7/10]. Step [1/104]. Loss: 0.010. r2: -0.029. Test r2: -0.237\n",
      "Epoch [7/10]. Step [31/104]. Loss: 0.009. r2: -0.194. Test r2: -0.230\n",
      "Epoch [7/10]. Step [61/104]. Loss: 0.009. r2: -0.225. Test r2: -0.239\n",
      "Epoch [7/10]. Step [91/104]. Loss: 0.009. r2: -0.411. Test r2: -0.219\n",
      "Epoch [8/10]. Step [1/104]. Loss: 0.010. r2: -0.016. Test r2: -0.235\n",
      "Epoch [8/10]. Step [31/104]. Loss: 0.009. r2: -0.311. Test r2: -0.228\n",
      "Epoch [8/10]. Step [61/104]. Loss: 0.009. r2: -0.344. Test r2: -0.226\n",
      "Epoch [8/10]. Step [91/104]. Loss: 0.009. r2: -0.296. Test r2: -0.225\n",
      "Epoch [9/10]. Step [1/104]. Loss: 0.010. r2: -0.018. Test r2: -0.236\n",
      "Epoch [9/10]. Step [31/104]. Loss: 0.009. r2: -0.202. Test r2: -0.228\n",
      "Epoch [9/10]. Step [61/104]. Loss: 0.009. r2: -0.322. Test r2: -0.230\n",
      "Epoch [9/10]. Step [91/104]. Loss: 0.009. r2: -0.257. Test r2: -0.224\n",
      "Epoch [10/10]. Step [1/104]. Loss: 0.010. r2: -0.019. Test r2: -0.238\n",
      "Epoch [10/10]. Step [31/104]. Loss: 0.009. r2: -0.326. Test r2: -0.228\n",
      "Epoch [10/10]. Step [61/104]. Loss: 0.009. r2: -0.278. Test r2: -0.228\n",
      "Epoch [10/10]. Step [91/104]. Loss: 0.009. r2: -0.354. Test r2: -0.229\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "train_nn(opt='sgd', num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель с оптимизатором Adam показала лучшие результаты."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
